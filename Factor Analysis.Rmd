---
title: "Factor Analysis"
author: "Wenxuan Zhang"
date: "8/11/2018"
contact: wz2270@columbia.edu
output: html_document
---

Factor Analysis is a frequently used apporach in marketing Research. The key idea behind factor analysis is that the variables are influenced by several independent factors. For example, we might have a data set that contains the marks of 9 diffrent subjects of n students and the psychologist believe that these variables are all influence by two factor, "verbal intelligence" and "math intelligence". The score of one subject, let us say, math, is determined by 10*verbal interlligence and 6* math inteligence, and the student veral intelligence is 6 and math intelligence is 6, then its mark it 96.  The coeffcient here are called factor loading  and math intelligence and verbal intelligence are called factors.

There are two types of frequently used factor analysis. EFA(Exploratory Factor Analysis) and CFA (Confirmatory Factor Analysis). EFA is more data driven and it tells you which factor are in a dataset and the researcher make no prior assumption among factors.

Comformatory Factor analysis, on the other hand assume there certian variables are correlated with certain factor. CFA utilized strucutral equation modeling to test whether hypothesis is true, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables.

The data set that we are going to leverage in our analysis is a product interest survey. And there are 11  questions regarding certain product asked and the answer rated at 7 point scales.

The 11 asked questions are:
* not important ( reversed)
* very interested 
* investigate in depth
* learn about options
* express person
* match one's image
* never think (reversed)
* look for specific features
* some are clearly better
* other see, form opinion
* tell about person

```{r}
library(corrplot)
pies.data <- read.csv("http://goo.gl/yT0XwJ")
summary(pies.data)

## vars n mean sd median trimmed mad min max range
## NotImportant 1 3600 4.34 1.00 4 4.32 1.48 1 7 6
## NeverThink 2 3600 4.10 1.05 4 4.09 1.48 1 7 6
## VeryInterested 3 3600 4.11 1.02 4 4.10 1.48 1 7 6
## LookFeatures 4 3600 4.04 1.05 4 4.04 1.48 1 7 6
## InvestigateDepth 5 3600 4.00 1.08 4 4.00 1.48 1 7 6
## SomeAreBetter 6 3600 3.92 1.04 4 3.94 1.48 1 7 6
## LearnAboutOptions 7 3600 3.87 1.04 4 3.88 1.48 1 7 6
## OthersOpinion 8 3600 3.90 1.11 4 3.92 1.48 1 7 6
## ExpressesPerson 9 3600 4.02 1.01 4 4.01 1.48 1 7 6
## TellsAbout 10 3600 3.90 1.02 4 3.92 1.48 1 7 6
## MatchImage

colnames(pies.data)<-c('NotImportant','NeverThink','VeryInterested','LookFeatures','InvestigateDepth','SomeAreBetter','LearnAboutOptions','OthersOpinion','ExpressesPerson','TellsAbout','MatchImage')
corrplot(cor(pies.data))
```

Then comes the question, how many factors should you use? There are several ways:
- Theory: how many do you expect?
- Correlation matrix: how many seem to be there?
- Eigenvalues: how many Factors have Eigenvalue > 1?
- Eigenvalue scree plot: where is the “bend” in extraction?
- Parallel analysis and acceleration [advanced; less used; not covered
today]

In factor analysis, an eigenvalue is the proportion of total shared
(i.e., non-error) variance explained by each factor. You might think
of it as volume in multidimensional space, where each variable adds
1.0 to the volume (thus, sum(eigenvalues) = # of variables).
A factor is only useful if it explains more than 1 variable . . . and
thus has eigenvalue > 1.0.
```{r}

eigen(cor(pies.data))$values
```
```{r}
plot(prcomp(pies.data),type="lines")
```
EFA can be thought of as slicing a pizza. The same material
(variance) can be carved up in ways that are mathematically
identical, but might be more or less useful for a given situation.
Key decision: do you want the extracted factors to be correlated or
not? In FA jargon, orthogonal or oblique?
By default, EFA looks for orthogonal factors that have r=0
correlation. This maximizes the interpretability, so I recommend
using an orthogonal rotation in most cases, at least to start. (As
a practical matter, it often makes little difference.)

Default: varimax: orthogonal rotation that aims for clear
factor/variable structure. Generally recommended.
Oblique: oblimin: finds correlated factors while aiming for
interpretability. Recommended if you want an oblique solution.
Oblique: promax: finds correlated factors similarly, but
computationally different (good alternative). Recommended
alternative if oblimin is not available or has difficulty.
many others . . . : dozens have been developed. IMO they are
useful mostly when you’re very concerned about psychometrics
(e.g., the College Board)

We decided 3 factors and we saw that these variables are clustered in 3 different factors.

```{r}
library(psych)
pies.fa <- fa(pies.data, nfactors=3, rotate="varimax")
pies.fa
```
```{r}
fa.diagram(pies.fa)
```
```{r}
fa.scores <- data.frame(pies.fa$scores)
names(fa.scores) <- c("ImageF", "FeatureF", "GeneralF")
head(fa.scores)
```
```{r}
# difference between PCA, pca make things hard to iterpret
princomp(pies.data)$loadings[ , 1:3]
# without rotation, it is difficult to interpret; most items load on 2 factors.
print(fa(pies.data, nfactors=3, rotate="none")$loadings, cut=0.3)
# with rotation
print(fa(pies.data, nfactors=3, rotate="varimax")$loadings, cut=0.3)
```

Confirmtoray Factor Analysis usually applied to evalute whether certain factorization is better than others. 

Define your hypothesized/favored model with relationships of
latent variables to manifest variables.
2 Define 1 or more alternative models that are reasonable, but
which you believe are inferior.
3 Fit the models to your data.
4 Determine whether your model is good enough (fit indices,
paths)
5 Determine whether your model is better than the alternative
6 Intepret your model (Optional: do a little dance. You deserve
it!)

CFA model is evaluated by the following criterias:

1.Global fit indices
Example: Comparative Fit Index (CFI). Attempts to assess
“absolute” fit vs. the data. Not very good measures, but set a
minimum bar: want fit > 0.90. Usually 0.9 is acceptable and 0.95 is considered as good.

2.Approximation error and residuals
Example: Standardized Root Mean Square Residual (SRMR).
Difference between the data’s covariance matrix and the fitted
model’s matrix. Want SRMR < 0.08. For Root Mean Square Error
of Approximation, want Lower-CI(RMSEA) < 0.05.

3.Information Criteria
Example: Akaike Information Criterion (AIC). Assesses the model’s
fit vs. the observed data. No absolute interpretation, but lower is
better. Difference of 10 or more is large.


```{r}
```
# Reference
1. <http://r-marketing.r-forge.r-project.org/Instructor/Intro%20Factor%20Analysis/intro-factor-analysis.pdf>
2. <https://en.wikipedia.org/wiki/Factor_analysis>
3. <https://www.statmethods.net/advstats/factor.html>
4. <https://en.wikipedia.org/wiki/Confirmatory_factor_analysis>
5.<https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf>
6. <https://people.ucsc.edu/~zurbrigg/psy214b/09SEM5a.pdf>